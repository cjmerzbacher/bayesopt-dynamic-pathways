{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Required packages\n",
    "import numpy as np\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from scikits.odes.ode import ode\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Color palette\n",
    "orders = ['No Control', 'Upstream Repression', 'Downstream Activation', 'Dual Control']\n",
    "palette = {'No Control': sns.color_palette()[3], 'Upstream Repression': 'tab:orange', 'Downstream Activation': 'tab:green', 'Dual Control': 'tab:blue', 'Initial':'black'}\n",
    "\n",
    "###Helper functions\n",
    "def loss_biological(j1, j2, alpha1=1E-5, alpha2=1E-2):\n",
    "        \"\"\"Computes scalarized loss including genetic constraint and product production\"\"\"\n",
    "        loss = alpha1*j1 + alpha2*j2\n",
    "        return j1, j2, loss\n",
    "\n",
    "def activation(x, k, theta, n):\n",
    "    return (k*(x/theta)**n)/(1+(x/theta)**n)\n",
    "\n",
    "def repression(x, k, theta, n):\n",
    "    return k/(1+(x/theta)**n)\n",
    "    \n",
    "def nonlinearity(x, kc, km):\n",
    "    return (kc*x)/(km+x)\n",
    "\n",
    "def name_converter(A):\n",
    "    if A == ((0, 1, 0), (1, 0, 0)):\n",
    "        return 'Dual Control'\n",
    "\n",
    "    elif A == ((0, 0, 1), (0, 0, 1)):\n",
    "        return 'No Control'\n",
    "\n",
    "    elif A == ((0, 0, 1), (1, 0, 0)):\n",
    "        return 'Downstream Activation'\n",
    "\n",
    "    elif A == ((0, 1, 0), (0, 0, 1)):\n",
    "        return 'Upstream Repression'\n",
    "    else: return 'Invalid Circuit'\n",
    "\n",
    "###Model definition\n",
    "def toy_model(t, y, ydot, params):\n",
    "    kc=12.; km=10.; lam=1.93E-4; Vin=1.; e0=0.0467\n",
    "    T = 1; E = 2; X = 2\n",
    "    A, W = params\n",
    "    ydot[0] = Vin - lam*y[0] - e0*nonlinearity(y[0], kc, km) - y[2]*nonlinearity(y[0], kc, km)\n",
    "    ydot[1] = y[2]*nonlinearity(y[0], kc, km) - y[3]*nonlinearity(y[1], kc, km) - lam*y[1]\n",
    "    for e in range(E):\n",
    "        ydot[e+X] = -lam*y[e+X] + np.sum(A[e]*np.array([activation(y[T], W[e][2], W[e][1], W[e][0]), repression(y[T], W[e][2], W[e][1], W[e][0]), W[e][2]]))\n",
    "    ydot[E+X] = (Vin - y[X+1]*nonlinearity(y[X-1], kc, km))**2 #J1\n",
    "    ydot[E+X+1] = np.sum([np.sum(A[e]*np.array([activation(y[T], W[e][2], W[e][1], W[e][0]), repression(y[T], W[e][2], W[e][1], W[e][0]), W[e][2]])) for e in range(E)]) #J2\n",
    "\n",
    "###Search space definition\n",
    "space = hp.choice('architecture', \n",
    "    [([[0, 0, 1], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]]),\n",
    "     ([[0, 1, 0], [0, 0, 1]], [[2., hp.uniform('theta1_ur', 0.001, 10), hp.uniform('k1_ur', 1E-7, 1E-3)], [2., hp.uniform('theta2_ur', 0.001, 10), hp.uniform('k2_ur', 1E-7, 1E-3)]]),\n",
    "     ([[0, 0, 1], [0, 0, 1]], [[2., hp.uniform('theta1_nc', 0.001, 10), hp.uniform('k1_nc', 1E-7, 1E-3)], [2., hp.uniform('theta2_nc', 0.001, 10), hp.uniform('k2_nc', 1E-7, 1E-3)]]),\n",
    "     ([[0, 1, 0], [1, 0, 0]], [[2., hp.uniform('theta1_dc', 0.001, 10), hp.uniform('k1_dc', 1E-7, 1E-3)], [2., hp.uniform('theta2_dc', 0.001, 10), hp.uniform('k2_dc', 1E-7, 1E-3)]])])\n",
    "    \n",
    "\n",
    "###Objective function\n",
    "def run_hyperopt(max_iters):\n",
    "    losses = []\n",
    "    params = []\n",
    "    circuits = []\n",
    "\n",
    "    #Define objective function\n",
    "    def objective(args):\n",
    "        architecture, param_values = args\n",
    "        #Integration conditions\n",
    "        t = np.linspace(0, 5E4, 100) \n",
    "        y0 = np.array([2290., 0., 0., 0., 0., 0.])\n",
    "\n",
    "        extra_options = {'old_api': False, 'user_data': [architecture, param_values]}\n",
    "        ode_solver = ode('cvode', toy_model, **extra_options)\n",
    "        solution = ode_solver.solve(t, y0)\n",
    "        j1, j2 = solution.values.y[-1, -2:]\n",
    "        j1, j2, loss = loss_biological(j1, j2, alpha1=1E-5, alpha2=1E-2)\n",
    "\n",
    "        losses.append(loss)\n",
    "        params.append(param_values)\n",
    "        circuits.append(architecture)\n",
    "        return loss\n",
    "\n",
    "    #Run hyperopt call\n",
    "    best = fmin(objective, space, algo=tpe.suggest, max_evals=max_iters)\n",
    "    #Create trajectory data frame\n",
    "    landscape = pd.DataFrame({'Circuit':circuits, 'Loss': losses, 'k1': [params[i][0][2] for i in range(len(params))], 'k2': [params[i][1][2] for i in range(len(params))], 'theta1': [params[i][0][1] for i in range(len(params))], 'theta2': [params[i][1][1] for i in range(len(params))]})\n",
    "    landscape['Circuit'] = [name_converter(c) for c in landscape.Circuit]\n",
    "\n",
    "    landscape = landscape.reset_index()\n",
    "\n",
    "    best_loss = 1E5\n",
    "    best_circuit = 'Initial'\n",
    "    best_losses = []\n",
    "    best_losses_circuits = []\n",
    "    for i in range(len(landscape)):\n",
    "        if landscape.Loss[i] < best_loss:\n",
    "            best_loss = landscape.Loss[i]\n",
    "            best_circuit = landscape.Circuit[i]\n",
    "        best_losses.append(best_loss)\n",
    "        best_losses_circuits.append(best_circuit)\n",
    "    landscape['best_losses'] = best_losses\n",
    "    landscape['best_loss_circuit'] = best_losses_circuits\n",
    "    return landscape, best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:28<00:00, 17.60trial/s, best loss: 0.06496124236243211]\n"
     ]
    }
   ],
   "source": [
    "###Run single architecture search to impute landscape\n",
    "max_iters = 500\n",
    "landscape, best = run_hyperopt(max_iters)\n",
    "landscape.to_csv('sample_run.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 52/1000 [00:04<01:04, 14.76trial/s, best loss: 0.06604478726228458]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: <method-wrapper '__getattribute__' of EnumMeta object at 0x7f8b9a397750> returned a result with an error set\n",
      "\n",
      "[CVODE ERROR]  CVode\n",
      "  At t = 3464.93, the right-hand side routine failed in an unrecoverable manner.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 53/1000 [00:04<01:18, 12.05trial/s, best loss: 0.06604478726228458]\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<method-wrapper '__getattribute__' of EnumMeta object at 0x7f8b9a397750> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32mscikits/odes/sundials/cvode.pyx:175\u001b[0m, in \u001b[0;36mscikits.odes.sundials.cvode._rhsfn\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mscikits/odes/sundials/cvode.pyx:151\u001b[0m, in \u001b[0;36mscikits.odes.sundials.cvode.CV_WrapRhsFunction.evaluate\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/charlotte/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb Cell 3'\u001b[0m in \u001b[0;36mtoy_model\u001b[0;34m(t, y, ydot, params)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(E):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=41'>42</a>\u001b[0m     ydot[e\u001b[39m+\u001b[39mX] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mlam\u001b[39m*\u001b[39my[e\u001b[39m+\u001b[39mX] \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49msum(A[e]\u001b[39m*\u001b[39;49mnp\u001b[39m.\u001b[39;49marray([activation(y[T], W[e][\u001b[39m2\u001b[39;49m], W[e][\u001b[39m1\u001b[39;49m], W[e][\u001b[39m0\u001b[39;49m]), repression(y[T], W[e][\u001b[39m2\u001b[39;49m], W[e][\u001b[39m1\u001b[39;49m], W[e][\u001b[39m0\u001b[39;49m]), W[e][\u001b[39m2\u001b[39;49m]]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=42'>43</a>\u001b[0m ydot[E\u001b[39m+\u001b[39mX] \u001b[39m=\u001b[39m (Vin \u001b[39m-\u001b[39m y[X\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39mnonlinearity(y[X\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], kc, km))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m#J1\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2259\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2257\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> 2259\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m   2260\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/charlotte/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=1'>2</a>\u001b[0m max_iters \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=3'>4</a>\u001b[0m space \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mchoice(\u001b[39m'\u001b[39m\u001b[39marchitecture\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=4'>5</a>\u001b[0m     [([[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]], [[\u001b[39m2.\u001b[39m, hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mtheta1_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m10\u001b[39m), hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mk1_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1E-7\u001b[39m, \u001b[39m1E-3\u001b[39m)], [\u001b[39m2.\u001b[39m, hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mtheta2_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m10\u001b[39m), hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mk2_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1E-7\u001b[39m, \u001b[39m1E-3\u001b[39m)]])])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=5'>6</a>\u001b[0m da_landscape, best \u001b[39m=\u001b[39m run_hyperopt(max_iters)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=7'>8</a>\u001b[0m space \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mchoice(\u001b[39m'\u001b[39m\u001b[39marchitecture\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=8'>9</a>\u001b[0m     [([[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]], [[\u001b[39m2.\u001b[39m, hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mtheta1_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m10\u001b[39m), hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mk1_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1E-7\u001b[39m, \u001b[39m1E-3\u001b[39m)], [\u001b[39m2.\u001b[39m, hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mtheta2_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m10\u001b[39m), hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mk2_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1E-7\u001b[39m, \u001b[39m1E-3\u001b[39m)]])])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=9'>10</a>\u001b[0m ur_landscape, best \u001b[39m=\u001b[39m run_hyperopt(max_iters)\n",
      "\u001b[1;32m/Users/charlotte/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb Cell 3'\u001b[0m in \u001b[0;36mrun_hyperopt\u001b[0;34m(max_iters)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=75'>76</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=77'>78</a>\u001b[0m \u001b[39m#Run hyperopt call\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=78'>79</a>\u001b[0m best \u001b[39m=\u001b[39m fmin(objective, space, algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest, max_evals\u001b[39m=\u001b[39;49mmax_iters)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=79'>80</a>\u001b[0m \u001b[39m#Create trajectory data frame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=80'>81</a>\u001b[0m landscape \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mCircuit\u001b[39m\u001b[39m'\u001b[39m:circuits, \u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m: losses, \u001b[39m'\u001b[39m\u001b[39mk1\u001b[39m\u001b[39m'\u001b[39m: [params[i][\u001b[39m0\u001b[39m][\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(params))], \u001b[39m'\u001b[39m\u001b[39mk2\u001b[39m\u001b[39m'\u001b[39m: [params[i][\u001b[39m1\u001b[39m][\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(params))], \u001b[39m'\u001b[39m\u001b[39mtheta1\u001b[39m\u001b[39m'\u001b[39m: [params[i][\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(params))], \u001b[39m'\u001b[39m\u001b[39mtheta2\u001b[39m\u001b[39m'\u001b[39m: [params[i][\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(params))]})\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[1;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[1;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[1;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "\u001b[1;32m/Users/charlotte/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb Cell 3'\u001b[0m in \u001b[0;36mrun_hyperopt.<locals>.objective\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=66'>67</a>\u001b[0m extra_options \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mold_api\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mFalse\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39muser_data\u001b[39m\u001b[39m'\u001b[39m: [architecture, param_values]}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=67'>68</a>\u001b[0m ode_solver \u001b[39m=\u001b[39m ode(\u001b[39m'\u001b[39m\u001b[39mcvode\u001b[39m\u001b[39m'\u001b[39m, toy_model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_options)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=68'>69</a>\u001b[0m solution \u001b[39m=\u001b[39m ode_solver\u001b[39m.\u001b[39;49msolve(t, y0)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=69'>70</a>\u001b[0m j1, j2 \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39my[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=70'>71</a>\u001b[0m j1, j2, loss \u001b[39m=\u001b[39m loss_biological(j1, j2, alpha1\u001b[39m=\u001b[39m\u001b[39m1E-5\u001b[39m, alpha2\u001b[39m=\u001b[39m\u001b[39m1E-2\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/scikits/odes/ode.py:333\u001b[0m, in \u001b[0;36mode.solve\u001b[0;34m(self, tspan, y0)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msolve\u001b[39m(\u001b[39mself\u001b[39m, tspan, y0):\n\u001b[1;32m    291\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    Runs the solver.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_integrator\u001b[39m.\u001b[39;49msolve(tspan, y0)\n",
      "File \u001b[0;32mscikits/odes/sundials/cvode.pyx:1763\u001b[0m, in \u001b[0;36mscikits.odes.sundials.cvode.CVODE.solve\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/enum.py:384\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# simple value lookup\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__new__\u001b[39;49m(\u001b[39mcls\u001b[39m, value)\n\u001b[1;32m    385\u001b[0m \u001b[39m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_create_(\n\u001b[1;32m    387\u001b[0m         value,\n\u001b[1;32m    388\u001b[0m         names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m         start\u001b[39m=\u001b[39mstart,\n\u001b[1;32m    393\u001b[0m         )\n",
      "\u001b[0;31mSystemError\u001b[0m: <method-wrapper '__getattribute__' of EnumMeta object at 0x7f8b9a397750> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "###Run single architecture search to impute landscape\n",
    "max_iters = 1000\n",
    "\n",
    "space = hp.choice('architecture', \n",
    "    [([[0, 0, 1], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "da_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "space = hp.choice('architecture', \n",
    "    [([[0, 1, 0], [0, 0, 1]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "ur_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "space = hp.choice('architecture', \n",
    "    [([[0, 0, 1], [0, 0, 1]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "nc_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "space = hp.choice('architecture', \n",
    "    [([[0, 1, 0], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "dc_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "landscape = pd.concat([dc_landscape, nc_landscape, ur_landscape, da_landscape])\n",
    "landscape.to_csv('single_architecture_run.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 0, 1), (1, 0, 0)) architecture complete\n",
      "374.8047821521759\n",
      "((0, 1, 0), (0, 0, 1)) architecture complete\n",
      "394.9919009208679\n",
      "((0, 0, 1), (0, 0, 1)) architecture complete\n",
      "381.6670792102814\n",
      "((0, 1, 0), (1, 0, 0)) architecture complete\n",
      "380.6115012168884\n"
     ]
    }
   ],
   "source": [
    "#Run single forward optimization grid search to create landscapes\n",
    "import time\n",
    "\n",
    "architectures = [((0, 0, 1), (1, 0, 0)), ((0, 1, 0), (0, 0, 1)), ((0, 0, 1), (0, 0, 1)), ((0, 1, 0), (1, 0, 0))]\n",
    "k1s = np.linspace(1E-7, 1E-3, 10)\n",
    "k2s = np.linspace(1E-7, 1E-3, 10)\n",
    "theta1s = np.linspace(0.001, 10, 10)\n",
    "theta2s = np.linspace(0.001, 10, 10)\n",
    "t = np.linspace(0, 5E4, 100) \n",
    "y0 = np.array([2290., 0., 0., 0., 0., 0.]) \n",
    "\n",
    "landscape_grid = pd.DataFrame()\n",
    "for architecture in architectures:\n",
    "        start_time = time.time()\n",
    "        for k1 in k1s:\n",
    "                for k2 in k2s:\n",
    "                        for theta1 in theta1s:\n",
    "                                for theta2 in theta2s:\n",
    "                                        param_values = [[2., theta1, k1], [2., theta2, k2]]\n",
    "                                        extra_options = {'old_api': False, 'user_data': [architecture, param_values]}\n",
    "                                        ode_solver = ode('cvode', toy_model, **extra_options)\n",
    "                                        solution = ode_solver.solve(t, y0)\n",
    "                                        j1, j2 = solution.values.y[-1, -2:]\n",
    "                                        j1, j2, loss = loss_biological(j1, j2, alpha1=1E-5, alpha2=1E-2)\n",
    "                                        test = pd.DataFrame({'index':[0], 'architecture':name_converter(architecture), 'theta1':theta1, 'theta2':theta2, 'k1':k1, 'k2':k2, 'loss':loss})\n",
    "                                        landscape_grid = pd.concat([landscape_grid, test])\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(architecture, 'architecture complete')\n",
    "        print(elapsed_time)\n",
    "landscape_grid.to_csv('grid_search_landscape_times.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:28<00:00, 17.84trial/s, best loss: 0.0657172102934111] \n",
      "100%|██████████| 500/500 [00:27<00:00, 17.99trial/s, best loss: 0.06417090801802705]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.17trial/s, best loss: 0.06369723092788487]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.20trial/s, best loss: 0.06106083389848402] \n",
      "100%|██████████| 500/500 [00:27<00:00, 18.05trial/s, best loss: 0.0622176926692011] \n",
      "100%|██████████| 500/500 [00:28<00:00, 17.54trial/s, best loss: 0.060877206745204915]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.26trial/s, best loss: 0.06518617786563781]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.36trial/s, best loss: 0.06409800072477195]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.42trial/s, best loss: 0.06153414979770218] \n",
      "100%|██████████| 500/500 [00:29<00:00, 17.13trial/s, best loss: 0.0647884344199869]\n",
      "100%|██████████| 500/500 [00:27<00:00, 17.94trial/s, best loss: 0.06230107991273731]\n",
      "100%|██████████| 500/500 [00:29<00:00, 17.23trial/s, best loss: 0.06474454779811081]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.70trial/s, best loss: 0.0651553384261133] \n",
      "100%|██████████| 500/500 [00:28<00:00, 17.57trial/s, best loss: 0.06620488007551684]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.83trial/s, best loss: 0.06568141608106463]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.57trial/s, best loss: 0.06150906621406434] \n",
      "100%|██████████| 500/500 [00:26<00:00, 18.66trial/s, best loss: 0.064883089508306]  \n",
      "100%|██████████| 500/500 [00:26<00:00, 18.61trial/s, best loss: 0.06099636104592455]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.69trial/s, best loss: 0.06497872540083675]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.41trial/s, best loss: 0.06456312349001805]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.81trial/s, best loss: 0.062296577856953916]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.27trial/s, best loss: 0.06376365302413654]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.40trial/s, best loss: 0.06590160964744854]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.73trial/s, best loss: 0.06437989127856919]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.41trial/s, best loss: 0.06178651397131674]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.32trial/s, best loss: 0.06458458309644594]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.61trial/s, best loss: 0.06513588000701225]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.30trial/s, best loss: 0.06153805572248404]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.79trial/s, best loss: 0.06468675952979061]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.22trial/s, best loss: 0.06411078877458842]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.39trial/s, best loss: 0.06529817459968841]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.55trial/s, best loss: 0.06529768659988101]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.70trial/s, best loss: 0.06105246364997271]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.33trial/s, best loss: 0.06403803729810922]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.56trial/s, best loss: 0.06437502888034129]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.75trial/s, best loss: 0.06428288648251125]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.39trial/s, best loss: 0.06434922556636374]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.79trial/s, best loss: 0.06110732333243199]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.46trial/s, best loss: 0.0642966778998009] \n",
      "100%|██████████| 500/500 [00:26<00:00, 18.71trial/s, best loss: 0.06160606010020004]\n",
      "100%|██████████| 500/500 [00:27<00:00, 17.97trial/s, best loss: 0.0649333707914545]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.63trial/s, best loss: 0.061351567177596954]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.63trial/s, best loss: 0.06450548560547856]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.81trial/s, best loss: 0.06500181823691588]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.48trial/s, best loss: 0.06424752907294914]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.62trial/s, best loss: 0.06447709260319057]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.48trial/s, best loss: 0.06132732973781095]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.29trial/s, best loss: 0.06594698671144823]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.50trial/s, best loss: 0.06539766330334376]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.78trial/s, best loss: 0.06400252523914292]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.37trial/s, best loss: 0.06485077021952831]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.21trial/s, best loss: 0.0650841968349049] \n",
      "100%|██████████| 500/500 [00:28<00:00, 17.76trial/s, best loss: 0.06421455857155092]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.51trial/s, best loss: 0.06438793573560109]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.53trial/s, best loss: 0.061999332599600956]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.13trial/s, best loss: 0.06612801730607978]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.25trial/s, best loss: 0.06535022570674529]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.25trial/s, best loss: 0.06433785226483467]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.74trial/s, best loss: 0.06271494132069982]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.97trial/s, best loss: 0.06446751167605205]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.71trial/s, best loss: 0.06452485499419072]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.11trial/s, best loss: 0.0647132798493493] \n",
      "100%|██████████| 500/500 [00:28<00:00, 17.40trial/s, best loss: 0.06409799258688467]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.45trial/s, best loss: 0.0648013544040256]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.71trial/s, best loss: 0.06520238926294925]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.79trial/s, best loss: 0.06543086450808802]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.81trial/s, best loss: 0.06292891156843701]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.56trial/s, best loss: 0.064620833986409]  \n",
      "100%|██████████| 500/500 [00:27<00:00, 18.34trial/s, best loss: 0.06114243163304596]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.28trial/s, best loss: 0.06375011926558521]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.28trial/s, best loss: 0.06476146176371592]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.31trial/s, best loss: 0.0644397778370585] \n",
      "100%|██████████| 500/500 [00:27<00:00, 17.90trial/s, best loss: 0.06406114612941999]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.58trial/s, best loss: 0.06415627300273032]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.37trial/s, best loss: 0.06421313355508512]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.61trial/s, best loss: 0.064507379346774]  \n",
      "100%|██████████| 500/500 [00:27<00:00, 18.35trial/s, best loss: 0.06473337064755043]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.72trial/s, best loss: 0.06265239253777817]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.83trial/s, best loss: 0.064459850698809]  \n",
      "100%|██████████| 500/500 [00:28<00:00, 17.77trial/s, best loss: 0.06411350428735756]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.31trial/s, best loss: 0.06188430070810518]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.20trial/s, best loss: 0.06468939852177442]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.53trial/s, best loss: 0.06206357246378939]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.48trial/s, best loss: 0.06362984712917175]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.55trial/s, best loss: 0.06499818470689674]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.53trial/s, best loss: 0.06204760688692122]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.76trial/s, best loss: 0.0647792752277514] \n",
      "100%|██████████| 500/500 [00:27<00:00, 18.34trial/s, best loss: 0.06610622322655677]\n",
      "100%|██████████| 500/500 [00:28<00:00, 17.47trial/s, best loss: 0.06396154023052383]\n",
      "100%|██████████| 500/500 [00:27<00:00, 18.16trial/s, best loss: 0.06482934551826885]\n",
      "100%|██████████| 500/500 [00:27<00:00, 17.87trial/s, best loss: 0.06487436119557018]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.52trial/s, best loss: 0.06427125301127012]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.90trial/s, best loss: 0.06500539772436582]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.70trial/s, best loss: 0.06386619123665273]\n",
      "100%|██████████| 500/500 [00:26<00:00, 19.02trial/s, best loss: 0.06257876599718057]\n",
      "100%|██████████| 500/500 [00:25<00:00, 19.27trial/s, best loss: 0.061421034510892675]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.99trial/s, best loss: 0.06477462546446386]\n",
      "100%|██████████| 500/500 [00:25<00:00, 19.25trial/s, best loss: 0.061840991276527355]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.60trial/s, best loss: 0.06396556118025791]\n",
      "100%|██████████| 500/500 [00:26<00:00, 18.90trial/s, best loss: 0.06449462936052919]\n"
     ]
    }
   ],
   "source": [
    "###Run hyperopt 100 times to solve for background - all four architectures\n",
    "max_iters = 500\n",
    "total_background =  pd.DataFrame()\n",
    "perturbs = [1., 1., 1.]\n",
    "for i in range(100):\n",
    "    space = hp.choice('architecture', \n",
    "    [([[0, 0, 1], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]]),\n",
    "     ([[0, 1, 0], [0, 0, 1]], [[2., hp.uniform('theta1_ur', 0.001, 10), hp.uniform('k1_ur', 1E-7, 1E-3)], [2., hp.uniform('theta2_ur', 0.001, 10), hp.uniform('k2_ur', 1E-7, 1E-3)]]),\n",
    "     ([[0, 0, 1], [0, 0, 1]], [[2., hp.uniform('theta1_nc', 0.001, 10), hp.uniform('k1_nc', 1E-7, 1E-3)], [2., hp.uniform('theta2_nc', 0.001, 10), hp.uniform('k2_nc', 1E-7, 1E-3)]]),\n",
    "     ([[0, 1, 0], [1, 0, 0]], [[2., hp.uniform('theta1_dc', 0.001, 10), hp.uniform('k1_dc', 1E-7, 1E-3)], [2., hp.uniform('theta2_dc', 0.001, 10), hp.uniform('k2_dc', 1E-7, 1E-3)]])])\n",
    "    \n",
    "    landscape, best = run_hyperopt(max_iters)\n",
    "    total_background = pd.concat([landscape, total_background])\n",
    "    #background.to_csv('background.csv', mode='a')\n",
    "    \n",
    "total_background.to_csv('background_fourarchitectures.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:35<00:00, 14.17trial/s, best loss: 0.06103148639576414] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.71trial/s, best loss: 0.07036831828910871]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.19trial/s, best loss: 0.06275017715342955]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.73trial/s, best loss: 0.06349380937170622]\n",
      "100%|██████████| 500/500 [00:29<00:00, 17.06trial/s, best loss: 0.06131074186033447]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.00trial/s, best loss: 0.06990558117354523]\n",
      "100%|██████████| 500/500 [00:29<00:00, 17.06trial/s, best loss: 0.06365242540898831]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.45trial/s, best loss: 0.06397760301902534]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.78trial/s, best loss: 0.06148173066675851]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.33trial/s, best loss: 0.07005744100502974]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.30trial/s, best loss: 0.06257525310383472]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.38trial/s, best loss: 0.06389460780413514]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.93trial/s, best loss: 0.06120656461472182]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.13trial/s, best loss: 0.0700693536344811] \n",
      "100%|██████████| 500/500 [00:28<00:00, 17.41trial/s, best loss: 0.06436578524232608]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.82trial/s, best loss: 0.06500734143848036]\n",
      "100%|██████████| 500/500 [00:29<00:00, 17.02trial/s, best loss: 0.06129221090236975]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.98trial/s, best loss: 0.07001378270643294]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.50trial/s, best loss: 0.06268271894798315]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.54trial/s, best loss: 0.06388839564874726]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.83trial/s, best loss: 0.061388275732235875]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.52trial/s, best loss: 0.0698180669037097] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.13trial/s, best loss: 0.06252846971000338]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.82trial/s, best loss: 0.06452714473832494]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.48trial/s, best loss: 0.0609471895690844] \n",
      "100%|██████████| 500/500 [00:31<00:00, 15.73trial/s, best loss: 0.07004863648776563]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.48trial/s, best loss: 0.06339516123816179]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.29trial/s, best loss: 0.06487372111930659]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.66trial/s, best loss: 0.06143275335568093] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.08trial/s, best loss: 0.0701357250971744]\n",
      "100%|██████████| 500/500 [00:29<00:00, 17.19trial/s, best loss: 0.06293640741988497]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.18trial/s, best loss: 0.06437701020657588]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.83trial/s, best loss: 0.06321996673889277]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.63trial/s, best loss: 0.06991354011234435]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.55trial/s, best loss: 0.06303226430379186]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.76trial/s, best loss: 0.06485689574726328]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.25trial/s, best loss: 0.06140383174652886]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.96trial/s, best loss: 0.07024979300124086]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.76trial/s, best loss: 0.06375818287470779]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.53trial/s, best loss: 0.0651451606758271] \n",
      "100%|██████████| 500/500 [00:32<00:00, 15.33trial/s, best loss: 0.06173571183526179] \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.36trial/s, best loss: 0.0697040754014384] \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.39trial/s, best loss: 0.06392684121953915]\n",
      "100%|██████████| 500/500 [00:46<00:00, 10.64trial/s, best loss: 0.06416373408373917]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.45trial/s, best loss: 0.061420365595229384]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.83trial/s, best loss: 0.0700679240969338] \n",
      "100%|██████████| 500/500 [00:31<00:00, 15.75trial/s, best loss: 0.06259131793292882]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.00trial/s, best loss: 0.06437298440712212]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.05trial/s, best loss: 0.061135232662384964]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.24trial/s, best loss: 0.07026229077233048]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.84trial/s, best loss: 0.06302952651049448]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.40trial/s, best loss: 0.06507565558623511]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.89trial/s, best loss: 0.06178180731272159]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.07trial/s, best loss: 0.07003412636716053]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.64trial/s, best loss: 0.06316036904472108]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.36trial/s, best loss: 0.06435947817750258]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.19trial/s, best loss: 0.062450521377379206]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.95trial/s, best loss: 0.07042583537373599]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.89trial/s, best loss: 0.06338850236635052]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.21trial/s, best loss: 0.06473297831029831]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.26trial/s, best loss: 0.06210968975576983]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.70trial/s, best loss: 0.06978001792366982]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.94trial/s, best loss: 0.06291655206354388]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.96trial/s, best loss: 0.06376994201343922]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.49trial/s, best loss: 0.061633869937714456]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.99trial/s, best loss: 0.07012566963272164]\n",
      "100%|██████████| 500/500 [00:43<00:00, 11.52trial/s, best loss: 0.0627735060140028] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.14trial/s, best loss: 0.06514239373329707]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.11trial/s, best loss: 0.06143494815121191] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.13trial/s, best loss: 0.07011147143358573]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.31trial/s, best loss: 0.06249518654287212]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.23trial/s, best loss: 0.06428612239647313]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.32trial/s, best loss: 0.06258565228894516]\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.81trial/s, best loss: 0.07002066644815749]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.67trial/s, best loss: 0.062511973341801]  \n",
      "100%|██████████| 500/500 [00:38<00:00, 12.87trial/s, best loss: 0.06397463815470855]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.58trial/s, best loss: 0.062492508130310626]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.78trial/s, best loss: 0.06979900637287609]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.37trial/s, best loss: 0.06338229544046386]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.60trial/s, best loss: 0.06451458520977645]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.02trial/s, best loss: 0.06277129873004766]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.29trial/s, best loss: 0.07012203259759445]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.57trial/s, best loss: 0.06439426328128758]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.55trial/s, best loss: 0.06447705990427641]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.61trial/s, best loss: 0.06177679226107101]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.37trial/s, best loss: 0.0701158896812291] \n",
      "100%|██████████| 500/500 [00:35<00:00, 13.97trial/s, best loss: 0.0628118281251982] \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.36trial/s, best loss: 0.06475228208620604]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.57trial/s, best loss: 0.061629520188732634]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.46trial/s, best loss: 0.06984265714218293]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.87trial/s, best loss: 0.062477468020040644]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.98trial/s, best loss: 0.06460989404266196]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.12trial/s, best loss: 0.061641569106374985]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.33trial/s, best loss: 0.06987806569951019]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.15trial/s, best loss: 0.06248111413733014]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.20trial/s, best loss: 0.06454792043048753]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.92trial/s, best loss: 0.06117016553370138] \n",
      "100%|██████████| 500/500 [00:36<00:00, 13.72trial/s, best loss: 0.06982873381892582]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.54trial/s, best loss: 0.06401225813438594]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.79trial/s, best loss: 0.06419884336558904]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.14trial/s, best loss: 0.061739721672454353]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.87trial/s, best loss: 0.07006971307999606]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.72trial/s, best loss: 0.06436806260413824]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.30trial/s, best loss: 0.06514055604374935]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.28trial/s, best loss: 0.06139259432440839] \n",
      "100%|██████████| 500/500 [00:35<00:00, 13.98trial/s, best loss: 0.07171191760205717]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.51trial/s, best loss: 0.06270002241207022]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.06trial/s, best loss: 0.06389774690292296]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.98trial/s, best loss: 0.06234629285908102]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.09trial/s, best loss: 0.06993092225368758]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.45trial/s, best loss: 0.06261419151907033]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.48trial/s, best loss: 0.06503733687715473]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.58trial/s, best loss: 0.06185841536044796]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.85trial/s, best loss: 0.06988098616631372]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.47trial/s, best loss: 0.06338683640942233]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.37trial/s, best loss: 0.06400045532304922]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.00trial/s, best loss: 0.061467560821713445]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.95trial/s, best loss: 0.06971813137376745]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.70trial/s, best loss: 0.06336533623878764]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.42trial/s, best loss: 0.06472183370424338]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.96trial/s, best loss: 0.06201220442338383]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.42trial/s, best loss: 0.07010017143659605]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.73trial/s, best loss: 0.06279901395194878]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.52trial/s, best loss: 0.06450471958995868]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.01trial/s, best loss: 0.06216930982423979]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.45trial/s, best loss: 0.07000264221720598]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.43trial/s, best loss: 0.06323099095189821]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.56trial/s, best loss: 0.06396799851236885]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.79trial/s, best loss: 0.06316326250131898]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.00trial/s, best loss: 0.06998491963855055]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.08trial/s, best loss: 0.06437591104649093]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.63trial/s, best loss: 0.06445955027077796]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.09trial/s, best loss: 0.06254797005393439]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.28trial/s, best loss: 0.07077553961653027]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.39trial/s, best loss: 0.06319654853876577]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.69trial/s, best loss: 0.06482123730793315]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.43trial/s, best loss: 0.06136979346162318]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.28trial/s, best loss: 0.07012803344986765]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.81trial/s, best loss: 0.06373212580467634]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.62trial/s, best loss: 0.064428617998434]  \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.36trial/s, best loss: 0.061267694742496846]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.58trial/s, best loss: 0.07027251840021298]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.21trial/s, best loss: 0.06304649635689741]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.60trial/s, best loss: 0.06461443068332491]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.68trial/s, best loss: 0.06217477144378422]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.03trial/s, best loss: 0.07001032609299354]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.20trial/s, best loss: 0.06268339224316238]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.16trial/s, best loss: 0.06427355232535185]\n",
      "100%|██████████| 500/500 [00:45<00:00, 11.01trial/s, best loss: 0.06118877175637312]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.60trial/s, best loss: 0.06997502211014664]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.83trial/s, best loss: 0.06363812551135099]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.77trial/s, best loss: 0.06434645949292364]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.61trial/s, best loss: 0.06120275035408293]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.05trial/s, best loss: 0.0698253852678408] \n",
      "100%|██████████| 500/500 [00:41<00:00, 11.95trial/s, best loss: 0.0637945987767504] \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.26trial/s, best loss: 0.06446012930416312]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.02trial/s, best loss: 0.06132675001001726]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.09trial/s, best loss: 0.06997635629721274]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.22trial/s, best loss: 0.06310048722051818]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.17trial/s, best loss: 0.06445922744921719]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.64trial/s, best loss: 0.0616110031626788]  \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.27trial/s, best loss: 0.0698803474162095] \n",
      "100%|██████████| 500/500 [00:32<00:00, 15.19trial/s, best loss: 0.06317049981215417]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.88trial/s, best loss: 0.06477695071850423]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.25trial/s, best loss: 0.06149717324772492]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.37trial/s, best loss: 0.07005858524903494]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.68trial/s, best loss: 0.06256639314491749]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.47trial/s, best loss: 0.06424596311988226]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.24trial/s, best loss: 0.061487995221792026]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.35trial/s, best loss: 0.0701677892668982] \n",
      "100%|██████████| 500/500 [00:31<00:00, 15.93trial/s, best loss: 0.06355573404344479]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.78trial/s, best loss: 0.06475636635406454]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.42trial/s, best loss: 0.06133592713925852]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.65trial/s, best loss: 0.07006330729950555]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.87trial/s, best loss: 0.06279072295584327]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.23trial/s, best loss: 0.0648919081612097] \n",
      "100%|██████████| 500/500 [00:39<00:00, 12.70trial/s, best loss: 0.06124083289876849]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.07trial/s, best loss: 0.07032746629802125]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.97trial/s, best loss: 0.06336108681936756]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.23trial/s, best loss: 0.06576379844636579]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.55trial/s, best loss: 0.061584488130134285]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.74trial/s, best loss: 0.07077331844265473]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.44trial/s, best loss: 0.06285153349803266]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.77trial/s, best loss: 0.06470251820857917]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.25trial/s, best loss: 0.06110001753154981]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.92trial/s, best loss: 0.07003110993591266]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.05trial/s, best loss: 0.0646739548815639] \n",
      "100%|██████████| 500/500 [00:41<00:00, 12.19trial/s, best loss: 0.06397858177435946]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.59trial/s, best loss: 0.061134487948836774]\n",
      "100%|██████████| 500/500 [00:43<00:00, 11.46trial/s, best loss: 0.07012914845469378]\n",
      "100%|██████████| 500/500 [00:41<00:00, 11.94trial/s, best loss: 0.06264923810852895]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.43trial/s, best loss: 0.06492217246079587]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.48trial/s, best loss: 0.06145222637866295] \n",
      "100%|██████████| 500/500 [00:40<00:00, 12.30trial/s, best loss: 0.06981968498352184]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.70trial/s, best loss: 0.06264790363000161]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.09trial/s, best loss: 0.06491043580519006]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.95trial/s, best loss: 0.061995471982525904]\n",
      "100%|██████████| 500/500 [00:48<00:00, 10.24trial/s, best loss: 0.07004099482436897]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.79trial/s, best loss: 0.06383226138636787]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.95trial/s, best loss: 0.0634190362356001] \n",
      "100%|██████████| 500/500 [00:32<00:00, 15.24trial/s, best loss: 0.060981452970153435]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.98trial/s, best loss: 0.06989123434682054]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.67trial/s, best loss: 0.062508166180891]  \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.11trial/s, best loss: 0.06445979963707525]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.47trial/s, best loss: 0.061819361688025994]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.37trial/s, best loss: 0.07014322561150682]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.90trial/s, best loss: 0.06295696421097975]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.96trial/s, best loss: 0.06435387021167971]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.41trial/s, best loss: 0.06300193095723286]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.30trial/s, best loss: 0.0700643164740836] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.89trial/s, best loss: 0.06347884061782716]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.48trial/s, best loss: 0.06492957277152729]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.99trial/s, best loss: 0.0613613385272119] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.01trial/s, best loss: 0.06981914252276794]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.84trial/s, best loss: 0.06270397294394793]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.34trial/s, best loss: 0.06427960961712381]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.14trial/s, best loss: 0.061107134765549895]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.17trial/s, best loss: 0.07012532624110707]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.16trial/s, best loss: 0.0632293674409794] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.83trial/s, best loss: 0.064629963442426] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.98trial/s, best loss: 0.06143573987206681]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.53trial/s, best loss: 0.0701118003615333] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.25trial/s, best loss: 0.06286541861161943]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.35trial/s, best loss: 0.06450608051762904]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.28trial/s, best loss: 0.06100420593129836]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.06trial/s, best loss: 0.07010236277194118]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.99trial/s, best loss: 0.06258180075114589]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.31trial/s, best loss: 0.06524301954208446]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.33trial/s, best loss: 0.06151515457286816]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.94trial/s, best loss: 0.06998283765213854]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.22trial/s, best loss: 0.06262961192737201]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.13trial/s, best loss: 0.06428501049520954]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.44trial/s, best loss: 0.06421406431605918]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.30trial/s, best loss: 0.07007786567370128]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.52trial/s, best loss: 0.06470623519266311]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.73trial/s, best loss: 0.06459962727483784]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.54trial/s, best loss: 0.06155080634098089]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.42trial/s, best loss: 0.07020951704988646]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.11trial/s, best loss: 0.062477368469634885]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.72trial/s, best loss: 0.06421527658022208]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.17trial/s, best loss: 0.06116443853887769]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.70trial/s, best loss: 0.06996592440325897]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.67trial/s, best loss: 0.06347064447124615]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.68trial/s, best loss: 0.06452526097138468]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.47trial/s, best loss: 0.06158942076928649] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.26trial/s, best loss: 0.06984604104394276]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.97trial/s, best loss: 0.06310639896363027]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.72trial/s, best loss: 0.06466716962242743]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.47trial/s, best loss: 0.06204520857959255]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.44trial/s, best loss: 0.0700370126124192] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.08trial/s, best loss: 0.06265195673650066]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.19trial/s, best loss: 0.0653342045880472] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.11trial/s, best loss: 0.06262975725550182]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.69trial/s, best loss: 0.07067854864621527]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.29trial/s, best loss: 0.06282365660378583]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.38trial/s, best loss: 0.06487201650006394]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.48trial/s, best loss: 0.061217008505393355]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.63trial/s, best loss: 0.07014365924785397]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.09trial/s, best loss: 0.0635575878910678] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.03trial/s, best loss: 0.0649565672690682] \n",
      "100%|██████████| 500/500 [00:36<00:00, 13.84trial/s, best loss: 0.06215419831236722]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.73trial/s, best loss: 0.06981450815147755]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.22trial/s, best loss: 0.06284932667666468]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.83trial/s, best loss: 0.06468730943841201]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.43trial/s, best loss: 0.06137464419441509]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.55trial/s, best loss: 0.07241490698237493]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.75trial/s, best loss: 0.06288153435124047]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.27trial/s, best loss: 0.0638752369603479] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.85trial/s, best loss: 0.06303565912687753]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.34trial/s, best loss: 0.06995451434232625]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.33trial/s, best loss: 0.06258156888283739]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.04trial/s, best loss: 0.06444941176888744]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.22trial/s, best loss: 0.06194071022639738]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.09trial/s, best loss: 0.06972685592584701]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.16trial/s, best loss: 0.06251152049411515]\n",
      "100%|██████████| 500/500 [00:48<00:00, 10.26trial/s, best loss: 0.06377598028786018]\n",
      "100%|██████████| 500/500 [00:52<00:00,  9.49trial/s, best loss: 0.06138281642532638]\n",
      "100%|██████████| 500/500 [00:46<00:00, 10.73trial/s, best loss: 0.06981643049712719]\n",
      "100%|██████████| 500/500 [00:41<00:00, 12.13trial/s, best loss: 0.06327841634197438]\n",
      "100%|██████████| 500/500 [00:45<00:00, 11.04trial/s, best loss: 0.0644738312459418] \n",
      "100%|██████████| 500/500 [00:45<00:00, 11.06trial/s, best loss: 0.061444407669890566]\n",
      "100%|██████████| 500/500 [00:54<00:00,  9.25trial/s, best loss: 0.06996153965641863]\n",
      "100%|██████████| 500/500 [00:44<00:00, 11.35trial/s, best loss: 0.06321498892343363]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.28trial/s, best loss: 0.06478423560317649]\n",
      "100%|██████████| 500/500 [00:41<00:00, 12.07trial/s, best loss: 0.061231982232717364]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.55trial/s, best loss: 0.07009225638858635]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.18trial/s, best loss: 0.06282367707645714]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.89trial/s, best loss: 0.06461751016619098]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.57trial/s, best loss: 0.061133192100795854]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.76trial/s, best loss: 0.07049857966293815]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.98trial/s, best loss: 0.06376620701539804]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.22trial/s, best loss: 0.06345914193789931]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.40trial/s, best loss: 0.06117854513093769]\n",
      "100%|██████████| 500/500 [00:47<00:00, 10.59trial/s, best loss: 0.06995660945844495]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.28trial/s, best loss: 0.0631893417686786] \n",
      "100%|██████████| 500/500 [00:36<00:00, 13.62trial/s, best loss: 0.06412594235881186]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.93trial/s, best loss: 0.061238643541346546]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.08trial/s, best loss: 0.06975118188542931]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.49trial/s, best loss: 0.06354799677081645]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.01trial/s, best loss: 0.06421548314915665]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.62trial/s, best loss: 0.061147162399856875]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.09trial/s, best loss: 0.07109222932184184]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.01trial/s, best loss: 0.06268733498057523]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.61trial/s, best loss: 0.06400310981924913]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.33trial/s, best loss: 0.061317450514544077]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.03trial/s, best loss: 0.07001514998172176]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.60trial/s, best loss: 0.06425369787382573]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.28trial/s, best loss: 0.06450421919781027]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.24trial/s, best loss: 0.06167473194385151]\n",
      "100%|██████████| 500/500 [00:41<00:00, 11.96trial/s, best loss: 0.0699015380042563] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.27trial/s, best loss: 0.06259820854744924]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.77trial/s, best loss: 0.06436151069681896]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.39trial/s, best loss: 0.06134211118332596] \n",
      "100%|██████████| 500/500 [00:41<00:00, 12.07trial/s, best loss: 0.0699599720457495] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.28trial/s, best loss: 0.06269918802411925]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.45trial/s, best loss: 0.06374716178094185]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.77trial/s, best loss: 0.06220780283167762]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.91trial/s, best loss: 0.06990422606869015]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.80trial/s, best loss: 0.06334089996611471]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.20trial/s, best loss: 0.06451744470400203]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.44trial/s, best loss: 0.06196845205128135]\n",
      "100%|██████████| 500/500 [00:44<00:00, 11.16trial/s, best loss: 0.0698244231896893] \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.32trial/s, best loss: 0.0629897738149143] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.81trial/s, best loss: 0.06407408425354685]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.28trial/s, best loss: 0.06093128643789217] \n",
      "100%|██████████| 500/500 [00:40<00:00, 12.30trial/s, best loss: 0.06974932455718348]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.30trial/s, best loss: 0.06291668570542239]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.52trial/s, best loss: 0.06400072086098683]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.00trial/s, best loss: 0.06172341117489391]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.54trial/s, best loss: 0.07007103767416746]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.45trial/s, best loss: 0.06404806791432437]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.78trial/s, best loss: 0.06631628299748708]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.92trial/s, best loss: 0.06110933125267533]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.47trial/s, best loss: 0.06996400555392193]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.50trial/s, best loss: 0.06271396727198979]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.61trial/s, best loss: 0.0638835765227112] \n",
      "100%|██████████| 500/500 [00:30<00:00, 16.22trial/s, best loss: 0.06190233297834545]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.98trial/s, best loss: 0.07024807335526073]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.72trial/s, best loss: 0.06510406307946796]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.61trial/s, best loss: 0.06502364713237506]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.10trial/s, best loss: 0.060924043554086336]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.78trial/s, best loss: 0.06990357175424487]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.57trial/s, best loss: 0.06254272233977642]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.18trial/s, best loss: 0.06420914457181998]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.36trial/s, best loss: 0.06254094209388951]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.33trial/s, best loss: 0.07013362493202]   \n",
      "100%|██████████| 500/500 [00:31<00:00, 15.68trial/s, best loss: 0.06398535444008499]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.59trial/s, best loss: 0.0645020021862912] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.13trial/s, best loss: 0.06102420707246563]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.62trial/s, best loss: 0.06991560473563024]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.20trial/s, best loss: 0.06275763769966128]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.22trial/s, best loss: 0.06437067120412691]\n",
      "100%|██████████| 500/500 [00:41<00:00, 11.93trial/s, best loss: 0.06225368594573711]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.26trial/s, best loss: 0.07005428772738705]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.21trial/s, best loss: 0.06284894279002788]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.83trial/s, best loss: 0.06480819209119526]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.55trial/s, best loss: 0.06213282298665003]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.86trial/s, best loss: 0.07051824830015668]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.50trial/s, best loss: 0.06403287268143629]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.03trial/s, best loss: 0.06396081949661168]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.02trial/s, best loss: 0.06164191640265196]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.48trial/s, best loss: 0.07017980218120057]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.78trial/s, best loss: 0.06271407322131782]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.90trial/s, best loss: 0.06471972651329207]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.66trial/s, best loss: 0.06126702223314947] \n",
      "100%|██████████| 500/500 [00:31<00:00, 15.72trial/s, best loss: 0.07018375860602193]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.66trial/s, best loss: 0.06254328026137453]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.42trial/s, best loss: 0.06453058432473309]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.43trial/s, best loss: 0.06126011194773212]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.49trial/s, best loss: 0.0700546239706924] \n",
      "100%|██████████| 500/500 [00:29<00:00, 16.85trial/s, best loss: 0.06304381657484387]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.10trial/s, best loss: 0.06362986588400098]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.36trial/s, best loss: 0.06221354603742575]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.34trial/s, best loss: 0.06989673306625367]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.82trial/s, best loss: 0.06252874099515474]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.34trial/s, best loss: 0.06369886116655057]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.80trial/s, best loss: 0.06092349254019469] \n",
      "100%|██████████| 500/500 [00:32<00:00, 15.40trial/s, best loss: 0.06997775400435605]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.53trial/s, best loss: 0.06258232945798868]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.80trial/s, best loss: 0.06368556674727854]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.03trial/s, best loss: 0.06187168079908035] \n",
      "100%|██████████| 500/500 [00:34<00:00, 14.63trial/s, best loss: 0.06975167720761961]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.50trial/s, best loss: 0.06576050549595032]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.66trial/s, best loss: 0.06401293938706021]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.48trial/s, best loss: 0.06371358338352173]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.67trial/s, best loss: 0.07008393758642097]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.72trial/s, best loss: 0.06284006321680964]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.79trial/s, best loss: 0.06364025169928568]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.55trial/s, best loss: 0.06123498740620037]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.55trial/s, best loss: 0.07010194484503761]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.89trial/s, best loss: 0.06329207770126186]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.09trial/s, best loss: 0.06463259690685552]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.26trial/s, best loss: 0.06245857009988719]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.48trial/s, best loss: 0.06991578968043535]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.65trial/s, best loss: 0.0629354741257688] \n",
      "100%|██████████| 500/500 [00:31<00:00, 16.11trial/s, best loss: 0.06439680602160842]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.49trial/s, best loss: 0.0613318889278446]  \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.99trial/s, best loss: 0.07027560375058345]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.64trial/s, best loss: 0.06339877682986472]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.78trial/s, best loss: 0.06439194041413149]\n"
     ]
    }
   ],
   "source": [
    "###Run hyperopt 100 times to solve for background - single architectures\n",
    "max_iters = 500\n",
    "total_background =  pd.DataFrame()\n",
    "perturbs = [1., 1., 1.]\n",
    "for i in range(100):\n",
    "    space = hp.choice('architecture', \n",
    "    [([[0, 0, 1], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "    da_landscape, best = run_hyperopt(max_iters)\n",
    "    \n",
    "    space = hp.choice('architecture', \n",
    "        [([[0, 1, 0], [0, 0, 1]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "    ur_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "    space = hp.choice('architecture', \n",
    "        [([[0, 0, 1], [0, 0, 1]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "    nc_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "    space = hp.choice('architecture', \n",
    "        [([[0, 1, 0], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "    dc_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "    background = pd.concat([dc_landscape, nc_landscape, ur_landscape, da_landscape])\n",
    "    total_background = pd.concat([total_background, background])\n",
    "    #background.to_csv('background.csv', mode='a')\n",
    "    \n",
    "total_background.to_csv('background.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###FIGURE: Landscapes for each architecture from grid search\n",
    "total_background = pd.read_csv('grid_search_landscape.csv')\n",
    "k1s = total_background.k1.unique()\n",
    "k2s = total_background.k2.unique()\n",
    "theta1s = total_background.theta1.unique()\n",
    "theta2s = total_background.theta2.unique()\n",
    "\n",
    "fig, axs = plt.subplots(1,4,figsize=(8, 2))\n",
    "colors = ['Reds_r','Oranges_r','Greens_r', 'Blues_r']\n",
    "archs = ['No Control', 'Upstream Repression', 'Downstream Activation', 'Dual Control']\n",
    "singlecolors = ['red', 'orange', 'green', 'blue']\n",
    "xs = ['k1', 'k1', 'k2', 'k1']\n",
    "ys = ['k2', 'theta1', 'theta2', 'theta1']\n",
    "xs_actual = [k1s, k1s, k2s, k1s]\n",
    "ys_actual = [k2s, theta1s, theta2s, theta1s]\n",
    "xs_names = ['$k_1$', '$k_1$', '$k_2$', '$k_1$']\n",
    "ys_names = ['$k_2$', r'$\\theta_1$', r'$\\theta_2$', r'$\\theta_1$']\n",
    "for i in range(4):\n",
    "    landscape = total_background.loc[total_background.architecture == archs[i]]\n",
    "    loss_landscape = landscape.pivot_table(index=xs[i], columns=ys[i], values='loss').T.values\n",
    "    ax = axs[i]\n",
    "    contour = ax.contourf(xs_actual[i], ys_actual[i], loss_landscape, cmap=colors[i])\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(xs_names[i], fontsize=14)\n",
    "    ax.set_ylabel(ys_names[i], fontsize=14)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('landscapes_grid_search.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###FIGURE: Single landscape for InkScape posprocessing\n",
    "i = 3 #Change to vary choice of landscape \n",
    "fig, axs = plt.subplots(1,1,figsize=(8, 8))\n",
    "landscape = total_background.loc[total_background.architecture == orders[i]]\n",
    "loss_landscape = landscape.pivot_table(index=xs[i], columns=ys[i], values='loss').T.values\n",
    "ax = axs\n",
    "contour = ax.contourf(xs_actual[i], ys_actual[i], loss_landscape, cmap=colors[i])\n",
    "ax.set_xticklabels('')\n",
    "ax.set_yticklabels('')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "# ax.set_xlabel(xs_names[i], fontsize=44)\n",
    "# ax.set_ylabel(ys_names[i], fontsize=44)\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_edgecolor(None)\n",
    "fig.tight_layout()\n",
    "fig.savefig('single_landscape_grid_search'+orders[i]+'.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sundials')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07c32881fdddda18fc4efdca8ccb6859d747bae1937efa0776c98adbd36477b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
