{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Required packages\n",
    "import numpy as np\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from scikits.odes.ode import ode\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Color palette\n",
    "orders = ['No Control', 'Upstream Repression', 'Downstream Activation', 'Dual Control']\n",
    "palette = {'No Control': sns.color_palette()[3], 'Upstream Repression': 'tab:orange', 'Downstream Activation': 'tab:green', 'Dual Control': 'tab:blue', 'Initial':'black'}\n",
    "\n",
    "###Helper functions\n",
    "def loss_biological(j1, j2, alpha1=1E-5, alpha2=1E-2):\n",
    "        \"\"\"Computes scalarized loss including genetic constraint and product production\"\"\"\n",
    "        loss = alpha1*j1 + alpha2*j2\n",
    "        return j1, j2, loss\n",
    "\n",
    "def activation(x, k, theta, n):\n",
    "    return (k*(x/theta)**n)/(1+(x/theta)**n)\n",
    "\n",
    "def repression(x, k, theta, n):\n",
    "    return k/(1+(x/theta)**n)\n",
    "    \n",
    "def nonlinearity(x, kc, km):\n",
    "    return (kc*x)/(km+x)\n",
    "\n",
    "def name_converter(A):\n",
    "    if A == ((0, 1, 0), (1, 0, 0)):\n",
    "        return 'Dual Control'\n",
    "\n",
    "    elif A == ((0, 0, 1), (0, 0, 1)):\n",
    "        return 'No Control'\n",
    "\n",
    "    elif A == ((0, 0, 1), (1, 0, 0)):\n",
    "        return 'Downstream Activation'\n",
    "\n",
    "    elif A == ((0, 1, 0), (0, 0, 1)):\n",
    "        return 'Upstream Repression'\n",
    "    else: return 'Invalid Circuit'\n",
    "\n",
    "###Model definition\n",
    "def toy_model(t, y, ydot, params):\n",
    "    kc=12.; km=10.; lam=1.93E-4; Vin=1.; e0=0.0467\n",
    "    T = 1; E = 2; X = 2\n",
    "    A, W = params\n",
    "    ydot[0] = Vin - lam*y[0] - e0*nonlinearity(y[0], kc, km) - y[2]*nonlinearity(y[0], kc, km)\n",
    "    ydot[1] = y[2]*nonlinearity(y[0], kc, km) - y[3]*nonlinearity(y[1], kc, km) - lam*y[1]\n",
    "    for e in range(E):\n",
    "        ydot[e+X] = -lam*y[e+X] + np.sum(A[e]*np.array([activation(y[T], W[e][2], W[e][1], W[e][0]), repression(y[T], W[e][2], W[e][1], W[e][0]), W[e][2]]))\n",
    "    ydot[E+X] = (Vin - y[X+1]*nonlinearity(y[X-1], kc, km))**2 #J1\n",
    "    ydot[E+X+1] = np.sum([np.sum(A[e]*np.array([activation(y[T], W[e][2], W[e][1], W[e][0]), repression(y[T], W[e][2], W[e][1], W[e][0]), W[e][2]])) for e in range(E)]) #J2\n",
    "\n",
    "###Search space definition\n",
    "space = hp.choice('architecture', \n",
    "    [([[0, 0, 1], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]]),\n",
    "     ([[0, 1, 0], [0, 0, 1]], [[2., hp.uniform('theta1_ur', 0.001, 10), hp.uniform('k1_ur', 1E-7, 1E-3)], [2., hp.uniform('theta2_ur', 0.001, 10), hp.uniform('k2_ur', 1E-7, 1E-3)]]),\n",
    "     ([[0, 0, 1], [0, 0, 1]], [[2., hp.uniform('theta1_nc', 0.001, 10), hp.uniform('k1_nc', 1E-7, 1E-3)], [2., hp.uniform('theta2_nc', 0.001, 10), hp.uniform('k2_nc', 1E-7, 1E-3)]]),\n",
    "     ([[0, 1, 0], [1, 0, 0]], [[2., hp.uniform('theta1_dc', 0.001, 10), hp.uniform('k1_dc', 1E-7, 1E-3)], [2., hp.uniform('theta2_dc', 0.001, 10), hp.uniform('k2_dc', 1E-7, 1E-3)]])])\n",
    "    \n",
    "\n",
    "###Objective function\n",
    "def run_hyperopt(max_iters):\n",
    "    losses = []\n",
    "    params = []\n",
    "    circuits = []\n",
    "\n",
    "    #Define objective function\n",
    "    def objective(args):\n",
    "        architecture, param_values = args\n",
    "        #Integration conditions\n",
    "        t = np.linspace(0, 5E4, 100) \n",
    "        y0 = np.array([2290., 0., 0., 0., 0., 0.])\n",
    "\n",
    "        extra_options = {'old_api': False, 'user_data': [architecture, param_values]}\n",
    "        ode_solver = ode('cvode', toy_model, **extra_options)\n",
    "        solution = ode_solver.solve(t, y0)\n",
    "        j1, j2 = solution.values.y[-1, -2:]\n",
    "        j1, j2, loss = loss_biological(j1, j2, alpha1=1E-5, alpha2=1E-2)\n",
    "\n",
    "        losses.append(loss)\n",
    "        params.append(param_values)\n",
    "        circuits.append(architecture)\n",
    "        return loss\n",
    "\n",
    "    #Run hyperopt call\n",
    "    best = fmin(objective, space, algo=tpe.suggest, max_evals=max_iters)\n",
    "    #Create trajectory data frame\n",
    "    landscape = pd.DataFrame({'Circuit':circuits, 'Loss': losses, 'k1': [params[i][0][2] for i in range(len(params))], 'k2': [params[i][1][2] for i in range(len(params))], 'theta1': [params[i][0][1] for i in range(len(params))], 'theta2': [params[i][1][1] for i in range(len(params))]})\n",
    "    landscape['Circuit'] = [name_converter(c) for c in landscape.Circuit]\n",
    "\n",
    "    landscape = landscape.reset_index()\n",
    "\n",
    "    best_loss = 1E5\n",
    "    best_circuit = 'Initial'\n",
    "    best_losses = []\n",
    "    best_losses_circuits = []\n",
    "    for i in range(len(landscape)):\n",
    "        if landscape.Loss[i] < best_loss:\n",
    "            best_loss = landscape.Loss[i]\n",
    "            best_circuit = landscape.Circuit[i]\n",
    "        best_losses.append(best_loss)\n",
    "        best_losses_circuits.append(best_circuit)\n",
    "    landscape['best_losses'] = best_losses\n",
    "    landscape['best_loss_circuit'] = best_losses_circuits\n",
    "    return landscape, best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 52/1000 [00:04<01:04, 14.76trial/s, best loss: 0.06604478726228458]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: <method-wrapper '__getattribute__' of EnumMeta object at 0x7f8b9a397750> returned a result with an error set\n",
      "\n",
      "[CVODE ERROR]  CVode\n",
      "  At t = 3464.93, the right-hand side routine failed in an unrecoverable manner.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 53/1000 [00:04<01:18, 12.05trial/s, best loss: 0.06604478726228458]\n"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<method-wrapper '__getattribute__' of EnumMeta object at 0x7f8b9a397750> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32mscikits/odes/sundials/cvode.pyx:175\u001b[0m, in \u001b[0;36mscikits.odes.sundials.cvode._rhsfn\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mscikits/odes/sundials/cvode.pyx:151\u001b[0m, in \u001b[0;36mscikits.odes.sundials.cvode.CV_WrapRhsFunction.evaluate\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/charlotte/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb Cell 3'\u001b[0m in \u001b[0;36mtoy_model\u001b[0;34m(t, y, ydot, params)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=40'>41</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(E):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=41'>42</a>\u001b[0m     ydot[e\u001b[39m+\u001b[39mX] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mlam\u001b[39m*\u001b[39my[e\u001b[39m+\u001b[39mX] \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49msum(A[e]\u001b[39m*\u001b[39;49mnp\u001b[39m.\u001b[39;49marray([activation(y[T], W[e][\u001b[39m2\u001b[39;49m], W[e][\u001b[39m1\u001b[39;49m], W[e][\u001b[39m0\u001b[39;49m]), repression(y[T], W[e][\u001b[39m2\u001b[39;49m], W[e][\u001b[39m1\u001b[39;49m], W[e][\u001b[39m0\u001b[39;49m]), W[e][\u001b[39m2\u001b[39;49m]]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=42'>43</a>\u001b[0m ydot[E\u001b[39m+\u001b[39mX] \u001b[39m=\u001b[39m (Vin \u001b[39m-\u001b[39m y[X\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39mnonlinearity(y[X\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], kc, km))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m#J1\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2259\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2257\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> 2259\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m   2260\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/charlotte/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=1'>2</a>\u001b[0m max_iters \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=3'>4</a>\u001b[0m space \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mchoice(\u001b[39m'\u001b[39m\u001b[39marchitecture\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=4'>5</a>\u001b[0m     [([[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]], [[\u001b[39m2.\u001b[39m, hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mtheta1_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m10\u001b[39m), hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mk1_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1E-7\u001b[39m, \u001b[39m1E-3\u001b[39m)], [\u001b[39m2.\u001b[39m, hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mtheta2_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m10\u001b[39m), hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mk2_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1E-7\u001b[39m, \u001b[39m1E-3\u001b[39m)]])])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=5'>6</a>\u001b[0m da_landscape, best \u001b[39m=\u001b[39m run_hyperopt(max_iters)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=7'>8</a>\u001b[0m space \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mchoice(\u001b[39m'\u001b[39m\u001b[39marchitecture\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=8'>9</a>\u001b[0m     [([[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]], [[\u001b[39m2.\u001b[39m, hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mtheta1_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m10\u001b[39m), hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mk1_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1E-7\u001b[39m, \u001b[39m1E-3\u001b[39m)], [\u001b[39m2.\u001b[39m, hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mtheta2_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m10\u001b[39m), hp\u001b[39m.\u001b[39muniform(\u001b[39m'\u001b[39m\u001b[39mk2_da\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1E-7\u001b[39m, \u001b[39m1E-3\u001b[39m)]])])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000003?line=9'>10</a>\u001b[0m ur_landscape, best \u001b[39m=\u001b[39m run_hyperopt(max_iters)\n",
      "\u001b[1;32m/Users/charlotte/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb Cell 3'\u001b[0m in \u001b[0;36mrun_hyperopt\u001b[0;34m(max_iters)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=75'>76</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=77'>78</a>\u001b[0m \u001b[39m#Run hyperopt call\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=78'>79</a>\u001b[0m best \u001b[39m=\u001b[39m fmin(objective, space, algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest, max_evals\u001b[39m=\u001b[39;49mmax_iters)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=79'>80</a>\u001b[0m \u001b[39m#Create trajectory data frame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=80'>81</a>\u001b[0m landscape \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mCircuit\u001b[39m\u001b[39m'\u001b[39m:circuits, \u001b[39m'\u001b[39m\u001b[39mLoss\u001b[39m\u001b[39m'\u001b[39m: losses, \u001b[39m'\u001b[39m\u001b[39mk1\u001b[39m\u001b[39m'\u001b[39m: [params[i][\u001b[39m0\u001b[39m][\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(params))], \u001b[39m'\u001b[39m\u001b[39mk2\u001b[39m\u001b[39m'\u001b[39m: [params[i][\u001b[39m1\u001b[39m][\u001b[39m2\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(params))], \u001b[39m'\u001b[39m\u001b[39mtheta1\u001b[39m\u001b[39m'\u001b[39m: [params[i][\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(params))], \u001b[39m'\u001b[39m\u001b[39mtheta2\u001b[39m\u001b[39m'\u001b[39m: [params[i][\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(params))]})\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[1;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[1;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[1;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "\u001b[1;32m/Users/charlotte/Library/Mobile Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb Cell 3'\u001b[0m in \u001b[0;36mrun_hyperopt.<locals>.objective\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=66'>67</a>\u001b[0m extra_options \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mold_api\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mFalse\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39muser_data\u001b[39m\u001b[39m'\u001b[39m: [architecture, param_values]}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=67'>68</a>\u001b[0m ode_solver \u001b[39m=\u001b[39m ode(\u001b[39m'\u001b[39m\u001b[39mcvode\u001b[39m\u001b[39m'\u001b[39m, toy_model, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_options)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=68'>69</a>\u001b[0m solution \u001b[39m=\u001b[39m ode_solver\u001b[39m.\u001b[39;49msolve(t, y0)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=69'>70</a>\u001b[0m j1, j2 \u001b[39m=\u001b[39m solution\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39my[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/charlotte/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/PhD/Thesis/bayesopt-dynamic-pathways/toy_model/toy_model.ipynb#ch0000002?line=70'>71</a>\u001b[0m j1, j2, loss \u001b[39m=\u001b[39m loss_biological(j1, j2, alpha1\u001b[39m=\u001b[39m\u001b[39m1E-5\u001b[39m, alpha2\u001b[39m=\u001b[39m\u001b[39m1E-2\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/site-packages/scikits/odes/ode.py:333\u001b[0m, in \u001b[0;36mode.solve\u001b[0;34m(self, tspan, y0)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msolve\u001b[39m(\u001b[39mself\u001b[39m, tspan, y0):\n\u001b[1;32m    291\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m    Runs the solver.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \n\u001b[1;32m    332\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_integrator\u001b[39m.\u001b[39;49msolve(tspan, y0)\n",
      "File \u001b[0;32mscikits/odes/sundials/cvode.pyx:1763\u001b[0m, in \u001b[0;36mscikits.odes.sundials.cvode.CVODE.solve\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sundials/lib/python3.9/enum.py:384\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# simple value lookup\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__new__\u001b[39;49m(\u001b[39mcls\u001b[39m, value)\n\u001b[1;32m    385\u001b[0m \u001b[39m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_create_(\n\u001b[1;32m    387\u001b[0m         value,\n\u001b[1;32m    388\u001b[0m         names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m         start\u001b[39m=\u001b[39mstart,\n\u001b[1;32m    393\u001b[0m         )\n",
      "\u001b[0;31mSystemError\u001b[0m: <method-wrapper '__getattribute__' of EnumMeta object at 0x7f8b9a397750> returned a result with an error set"
     ]
    }
   ],
   "source": [
    "###Run single architecture search to impute landscape\n",
    "max_iters = 1000\n",
    "\n",
    "space = hp.choice('architecture', \n",
    "    [([[0, 0, 1], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "da_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "space = hp.choice('architecture', \n",
    "    [([[0, 1, 0], [0, 0, 1]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "ur_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "space = hp.choice('architecture', \n",
    "    [([[0, 0, 1], [0, 0, 1]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "nc_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "space = hp.choice('architecture', \n",
    "    [([[0, 1, 0], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "dc_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "landscape = pd.concat([dc_landscape, nc_landscape, ur_landscape, da_landscape])\n",
    "landscape.to_csv('single_architecture_run.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture complete\n",
      "architecture complete\n",
      "architecture complete\n",
      "architecture complete\n"
     ]
    }
   ],
   "source": [
    "#Run single forward optimization grid search to create landscapes\n",
    "architectures = [((0, 0, 1), (1, 0, 0)), ((0, 1, 0), (0, 0, 1)), ((0, 0, 1), (0, 0, 1)), ((0, 1, 0), (1, 0, 0))]\n",
    "k1s = np.linspace(1E-7, 1E-3, 10)\n",
    "k2s = np.linspace(1E-7, 1E-3, 10)\n",
    "theta1s = np.linspace(0.001, 10, 10)\n",
    "theta2s = np.linspace(0.001, 10, 10)\n",
    "t = np.linspace(0, 5E4, 100) \n",
    "y0 = np.array([2290., 0., 0., 0., 0., 0.])\n",
    "\n",
    "landscape_grid = pd.DataFrame()\n",
    "for architecture in architectures:\n",
    "        for k1 in k1s:\n",
    "                for k2 in k2s:\n",
    "                        for theta1 in theta1s:\n",
    "                                for theta2 in theta2s:\n",
    "                                        param_values = [[2., theta1, k1], [2., theta2, k2]]\n",
    "                                        extra_options = {'old_api': False, 'user_data': [architecture, param_values]}\n",
    "                                        ode_solver = ode('cvode', toy_model, **extra_options)\n",
    "                                        solution = ode_solver.solve(t, y0)\n",
    "                                        j1, j2 = solution.values.y[-1, -2:]\n",
    "                                        j1, j2, loss = loss_biological(j1, j2, alpha1=1E-5, alpha2=1E-2)\n",
    "                                        test = pd.DataFrame({'index':[0], 'architecture':name_converter(architecture), 'theta1':theta1, 'theta2':theta2, 'k1':k1, 'k2':k2, 'loss':loss})\n",
    "                                        landscape_grid = pd.concat([landscape_grid, test])\n",
    "        print('architecture complete')\n",
    "landscape_grid.to_csv('grid_search_landscape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:35<00:00, 14.17trial/s, best loss: 0.06103148639576414] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.71trial/s, best loss: 0.07036831828910871]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.19trial/s, best loss: 0.06275017715342955]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.73trial/s, best loss: 0.06349380937170622]\n",
      "100%|██████████| 500/500 [00:29<00:00, 17.06trial/s, best loss: 0.06131074186033447]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.00trial/s, best loss: 0.06990558117354523]\n",
      "100%|██████████| 500/500 [00:29<00:00, 17.06trial/s, best loss: 0.06365242540898831]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.45trial/s, best loss: 0.06397760301902534]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.78trial/s, best loss: 0.06148173066675851]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.33trial/s, best loss: 0.07005744100502974]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.30trial/s, best loss: 0.06257525310383472]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.38trial/s, best loss: 0.06389460780413514]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.93trial/s, best loss: 0.06120656461472182]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.13trial/s, best loss: 0.0700693536344811] \n",
      "100%|██████████| 500/500 [00:28<00:00, 17.41trial/s, best loss: 0.06436578524232608]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.82trial/s, best loss: 0.06500734143848036]\n",
      "100%|██████████| 500/500 [00:29<00:00, 17.02trial/s, best loss: 0.06129221090236975]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.98trial/s, best loss: 0.07001378270643294]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.50trial/s, best loss: 0.06268271894798315]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.54trial/s, best loss: 0.06388839564874726]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.83trial/s, best loss: 0.061388275732235875]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.52trial/s, best loss: 0.0698180669037097] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.13trial/s, best loss: 0.06252846971000338]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.82trial/s, best loss: 0.06452714473832494]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.48trial/s, best loss: 0.0609471895690844] \n",
      "100%|██████████| 500/500 [00:31<00:00, 15.73trial/s, best loss: 0.07004863648776563]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.48trial/s, best loss: 0.06339516123816179]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.29trial/s, best loss: 0.06487372111930659]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.66trial/s, best loss: 0.06143275335568093] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.08trial/s, best loss: 0.0701357250971744]\n",
      "100%|██████████| 500/500 [00:29<00:00, 17.19trial/s, best loss: 0.06293640741988497]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.18trial/s, best loss: 0.06437701020657588]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.83trial/s, best loss: 0.06321996673889277]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.63trial/s, best loss: 0.06991354011234435]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.55trial/s, best loss: 0.06303226430379186]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.76trial/s, best loss: 0.06485689574726328]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.25trial/s, best loss: 0.06140383174652886]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.96trial/s, best loss: 0.07024979300124086]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.76trial/s, best loss: 0.06375818287470779]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.53trial/s, best loss: 0.0651451606758271] \n",
      "100%|██████████| 500/500 [00:32<00:00, 15.33trial/s, best loss: 0.06173571183526179] \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.36trial/s, best loss: 0.0697040754014384] \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.39trial/s, best loss: 0.06392684121953915]\n",
      "100%|██████████| 500/500 [00:46<00:00, 10.64trial/s, best loss: 0.06416373408373917]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.45trial/s, best loss: 0.061420365595229384]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.83trial/s, best loss: 0.0700679240969338] \n",
      "100%|██████████| 500/500 [00:31<00:00, 15.75trial/s, best loss: 0.06259131793292882]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.00trial/s, best loss: 0.06437298440712212]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.05trial/s, best loss: 0.061135232662384964]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.24trial/s, best loss: 0.07026229077233048]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.84trial/s, best loss: 0.06302952651049448]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.40trial/s, best loss: 0.06507565558623511]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.89trial/s, best loss: 0.06178180731272159]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.07trial/s, best loss: 0.07003412636716053]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.64trial/s, best loss: 0.06316036904472108]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.36trial/s, best loss: 0.06435947817750258]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.19trial/s, best loss: 0.062450521377379206]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.95trial/s, best loss: 0.07042583537373599]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.89trial/s, best loss: 0.06338850236635052]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.21trial/s, best loss: 0.06473297831029831]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.26trial/s, best loss: 0.06210968975576983]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.70trial/s, best loss: 0.06978001792366982]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.94trial/s, best loss: 0.06291655206354388]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.96trial/s, best loss: 0.06376994201343922]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.49trial/s, best loss: 0.061633869937714456]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.99trial/s, best loss: 0.07012566963272164]\n",
      "100%|██████████| 500/500 [00:43<00:00, 11.52trial/s, best loss: 0.0627735060140028] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.14trial/s, best loss: 0.06514239373329707]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.11trial/s, best loss: 0.06143494815121191] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.13trial/s, best loss: 0.07011147143358573]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.31trial/s, best loss: 0.06249518654287212]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.23trial/s, best loss: 0.06428612239647313]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.32trial/s, best loss: 0.06258565228894516]\n",
      "100%|██████████| 500/500 [00:42<00:00, 11.81trial/s, best loss: 0.07002066644815749]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.67trial/s, best loss: 0.062511973341801]  \n",
      "100%|██████████| 500/500 [00:38<00:00, 12.87trial/s, best loss: 0.06397463815470855]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.58trial/s, best loss: 0.062492508130310626]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.78trial/s, best loss: 0.06979900637287609]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.37trial/s, best loss: 0.06338229544046386]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.60trial/s, best loss: 0.06451458520977645]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.02trial/s, best loss: 0.06277129873004766]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.29trial/s, best loss: 0.07012203259759445]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.57trial/s, best loss: 0.06439426328128758]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.55trial/s, best loss: 0.06447705990427641]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.61trial/s, best loss: 0.06177679226107101]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.37trial/s, best loss: 0.0701158896812291] \n",
      "100%|██████████| 500/500 [00:35<00:00, 13.97trial/s, best loss: 0.0628118281251982] \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.36trial/s, best loss: 0.06475228208620604]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.57trial/s, best loss: 0.061629520188732634]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.46trial/s, best loss: 0.06984265714218293]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.87trial/s, best loss: 0.062477468020040644]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.98trial/s, best loss: 0.06460989404266196]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.12trial/s, best loss: 0.061641569106374985]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.33trial/s, best loss: 0.06987806569951019]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.15trial/s, best loss: 0.06248111413733014]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.20trial/s, best loss: 0.06454792043048753]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.92trial/s, best loss: 0.06117016553370138] \n",
      "100%|██████████| 500/500 [00:36<00:00, 13.72trial/s, best loss: 0.06982873381892582]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.54trial/s, best loss: 0.06401225813438594]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.79trial/s, best loss: 0.06419884336558904]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.14trial/s, best loss: 0.061739721672454353]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.87trial/s, best loss: 0.07006971307999606]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.72trial/s, best loss: 0.06436806260413824]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.30trial/s, best loss: 0.06514055604374935]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.28trial/s, best loss: 0.06139259432440839] \n",
      "100%|██████████| 500/500 [00:35<00:00, 13.98trial/s, best loss: 0.07171191760205717]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.51trial/s, best loss: 0.06270002241207022]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.06trial/s, best loss: 0.06389774690292296]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.98trial/s, best loss: 0.06234629285908102]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.09trial/s, best loss: 0.06993092225368758]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.45trial/s, best loss: 0.06261419151907033]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.48trial/s, best loss: 0.06503733687715473]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.58trial/s, best loss: 0.06185841536044796]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.85trial/s, best loss: 0.06988098616631372]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.47trial/s, best loss: 0.06338683640942233]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.37trial/s, best loss: 0.06400045532304922]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.00trial/s, best loss: 0.061467560821713445]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.95trial/s, best loss: 0.06971813137376745]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.70trial/s, best loss: 0.06336533623878764]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.42trial/s, best loss: 0.06472183370424338]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.96trial/s, best loss: 0.06201220442338383]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.42trial/s, best loss: 0.07010017143659605]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.73trial/s, best loss: 0.06279901395194878]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.52trial/s, best loss: 0.06450471958995868]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.01trial/s, best loss: 0.06216930982423979]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.45trial/s, best loss: 0.07000264221720598]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.43trial/s, best loss: 0.06323099095189821]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.56trial/s, best loss: 0.06396799851236885]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.79trial/s, best loss: 0.06316326250131898]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.00trial/s, best loss: 0.06998491963855055]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.08trial/s, best loss: 0.06437591104649093]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.63trial/s, best loss: 0.06445955027077796]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.09trial/s, best loss: 0.06254797005393439]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.28trial/s, best loss: 0.07077553961653027]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.39trial/s, best loss: 0.06319654853876577]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.69trial/s, best loss: 0.06482123730793315]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.43trial/s, best loss: 0.06136979346162318]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.28trial/s, best loss: 0.07012803344986765]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.81trial/s, best loss: 0.06373212580467634]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.62trial/s, best loss: 0.064428617998434]  \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.36trial/s, best loss: 0.061267694742496846]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.58trial/s, best loss: 0.07027251840021298]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.21trial/s, best loss: 0.06304649635689741]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.60trial/s, best loss: 0.06461443068332491]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.68trial/s, best loss: 0.06217477144378422]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.03trial/s, best loss: 0.07001032609299354]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.20trial/s, best loss: 0.06268339224316238]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.16trial/s, best loss: 0.06427355232535185]\n",
      "100%|██████████| 500/500 [00:45<00:00, 11.01trial/s, best loss: 0.06118877175637312]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.60trial/s, best loss: 0.06997502211014664]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.83trial/s, best loss: 0.06363812551135099]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.77trial/s, best loss: 0.06434645949292364]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.61trial/s, best loss: 0.06120275035408293]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.05trial/s, best loss: 0.0698253852678408] \n",
      "100%|██████████| 500/500 [00:41<00:00, 11.95trial/s, best loss: 0.0637945987767504] \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.26trial/s, best loss: 0.06446012930416312]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.02trial/s, best loss: 0.06132675001001726]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.09trial/s, best loss: 0.06997635629721274]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.22trial/s, best loss: 0.06310048722051818]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.17trial/s, best loss: 0.06445922744921719]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.64trial/s, best loss: 0.0616110031626788]  \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.27trial/s, best loss: 0.0698803474162095] \n",
      "100%|██████████| 500/500 [00:32<00:00, 15.19trial/s, best loss: 0.06317049981215417]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.88trial/s, best loss: 0.06477695071850423]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.25trial/s, best loss: 0.06149717324772492]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.37trial/s, best loss: 0.07005858524903494]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.68trial/s, best loss: 0.06256639314491749]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.47trial/s, best loss: 0.06424596311988226]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.24trial/s, best loss: 0.061487995221792026]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.35trial/s, best loss: 0.0701677892668982] \n",
      "100%|██████████| 500/500 [00:31<00:00, 15.93trial/s, best loss: 0.06355573404344479]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.78trial/s, best loss: 0.06475636635406454]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.42trial/s, best loss: 0.06133592713925852]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.65trial/s, best loss: 0.07006330729950555]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.87trial/s, best loss: 0.06279072295584327]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.23trial/s, best loss: 0.0648919081612097] \n",
      "100%|██████████| 500/500 [00:39<00:00, 12.70trial/s, best loss: 0.06124083289876849]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.07trial/s, best loss: 0.07032746629802125]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.97trial/s, best loss: 0.06336108681936756]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.23trial/s, best loss: 0.06576379844636579]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.55trial/s, best loss: 0.061584488130134285]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.74trial/s, best loss: 0.07077331844265473]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.44trial/s, best loss: 0.06285153349803266]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.77trial/s, best loss: 0.06470251820857917]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.25trial/s, best loss: 0.06110001753154981]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.92trial/s, best loss: 0.07003110993591266]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.05trial/s, best loss: 0.0646739548815639] \n",
      "100%|██████████| 500/500 [00:41<00:00, 12.19trial/s, best loss: 0.06397858177435946]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.59trial/s, best loss: 0.061134487948836774]\n",
      "100%|██████████| 500/500 [00:43<00:00, 11.46trial/s, best loss: 0.07012914845469378]\n",
      "100%|██████████| 500/500 [00:41<00:00, 11.94trial/s, best loss: 0.06264923810852895]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.43trial/s, best loss: 0.06492217246079587]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.48trial/s, best loss: 0.06145222637866295] \n",
      "100%|██████████| 500/500 [00:40<00:00, 12.30trial/s, best loss: 0.06981968498352184]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.70trial/s, best loss: 0.06264790363000161]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.09trial/s, best loss: 0.06491043580519006]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.95trial/s, best loss: 0.061995471982525904]\n",
      "100%|██████████| 500/500 [00:48<00:00, 10.24trial/s, best loss: 0.07004099482436897]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.79trial/s, best loss: 0.06383226138636787]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.95trial/s, best loss: 0.0634190362356001] \n",
      "100%|██████████| 500/500 [00:32<00:00, 15.24trial/s, best loss: 0.060981452970153435]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.98trial/s, best loss: 0.06989123434682054]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.67trial/s, best loss: 0.062508166180891]  \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.11trial/s, best loss: 0.06445979963707525]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.47trial/s, best loss: 0.061819361688025994]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.37trial/s, best loss: 0.07014322561150682]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.90trial/s, best loss: 0.06295696421097975]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.96trial/s, best loss: 0.06435387021167971]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.41trial/s, best loss: 0.06300193095723286]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.30trial/s, best loss: 0.0700643164740836] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.89trial/s, best loss: 0.06347884061782716]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.48trial/s, best loss: 0.06492957277152729]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.99trial/s, best loss: 0.0613613385272119] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.01trial/s, best loss: 0.06981914252276794]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.84trial/s, best loss: 0.06270397294394793]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.34trial/s, best loss: 0.06427960961712381]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.14trial/s, best loss: 0.061107134765549895]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.17trial/s, best loss: 0.07012532624110707]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.16trial/s, best loss: 0.0632293674409794] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.83trial/s, best loss: 0.064629963442426] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.98trial/s, best loss: 0.06143573987206681]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.53trial/s, best loss: 0.0701118003615333] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.25trial/s, best loss: 0.06286541861161943]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.35trial/s, best loss: 0.06450608051762904]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.28trial/s, best loss: 0.06100420593129836]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.06trial/s, best loss: 0.07010236277194118]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.99trial/s, best loss: 0.06258180075114589]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.31trial/s, best loss: 0.06524301954208446]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.33trial/s, best loss: 0.06151515457286816]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.94trial/s, best loss: 0.06998283765213854]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.22trial/s, best loss: 0.06262961192737201]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.13trial/s, best loss: 0.06428501049520954]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.44trial/s, best loss: 0.06421406431605918]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.30trial/s, best loss: 0.07007786567370128]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.52trial/s, best loss: 0.06470623519266311]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.73trial/s, best loss: 0.06459962727483784]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.54trial/s, best loss: 0.06155080634098089]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.42trial/s, best loss: 0.07020951704988646]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.11trial/s, best loss: 0.062477368469634885]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.72trial/s, best loss: 0.06421527658022208]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.17trial/s, best loss: 0.06116443853887769]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.70trial/s, best loss: 0.06996592440325897]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.67trial/s, best loss: 0.06347064447124615]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.68trial/s, best loss: 0.06452526097138468]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.47trial/s, best loss: 0.06158942076928649] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.26trial/s, best loss: 0.06984604104394276]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.97trial/s, best loss: 0.06310639896363027]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.72trial/s, best loss: 0.06466716962242743]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.47trial/s, best loss: 0.06204520857959255]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.44trial/s, best loss: 0.0700370126124192] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.08trial/s, best loss: 0.06265195673650066]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.19trial/s, best loss: 0.0653342045880472] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.11trial/s, best loss: 0.06262975725550182]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.69trial/s, best loss: 0.07067854864621527]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.29trial/s, best loss: 0.06282365660378583]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.38trial/s, best loss: 0.06487201650006394]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.48trial/s, best loss: 0.061217008505393355]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.63trial/s, best loss: 0.07014365924785397]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.09trial/s, best loss: 0.0635575878910678] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.03trial/s, best loss: 0.0649565672690682] \n",
      "100%|██████████| 500/500 [00:36<00:00, 13.84trial/s, best loss: 0.06215419831236722]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.73trial/s, best loss: 0.06981450815147755]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.22trial/s, best loss: 0.06284932667666468]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.83trial/s, best loss: 0.06468730943841201]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.43trial/s, best loss: 0.06137464419441509]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.55trial/s, best loss: 0.07241490698237493]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.75trial/s, best loss: 0.06288153435124047]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.27trial/s, best loss: 0.0638752369603479] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.85trial/s, best loss: 0.06303565912687753]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.34trial/s, best loss: 0.06995451434232625]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.33trial/s, best loss: 0.06258156888283739]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.04trial/s, best loss: 0.06444941176888744]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.22trial/s, best loss: 0.06194071022639738]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.09trial/s, best loss: 0.06972685592584701]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.16trial/s, best loss: 0.06251152049411515]\n",
      "100%|██████████| 500/500 [00:48<00:00, 10.26trial/s, best loss: 0.06377598028786018]\n",
      "100%|██████████| 500/500 [00:52<00:00,  9.49trial/s, best loss: 0.06138281642532638]\n",
      "100%|██████████| 500/500 [00:46<00:00, 10.73trial/s, best loss: 0.06981643049712719]\n",
      "100%|██████████| 500/500 [00:41<00:00, 12.13trial/s, best loss: 0.06327841634197438]\n",
      "100%|██████████| 500/500 [00:45<00:00, 11.04trial/s, best loss: 0.0644738312459418] \n",
      "100%|██████████| 500/500 [00:45<00:00, 11.06trial/s, best loss: 0.061444407669890566]\n",
      "100%|██████████| 500/500 [00:54<00:00,  9.25trial/s, best loss: 0.06996153965641863]\n",
      "100%|██████████| 500/500 [00:44<00:00, 11.35trial/s, best loss: 0.06321498892343363]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.28trial/s, best loss: 0.06478423560317649]\n",
      "100%|██████████| 500/500 [00:41<00:00, 12.07trial/s, best loss: 0.061231982232717364]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.55trial/s, best loss: 0.07009225638858635]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.18trial/s, best loss: 0.06282367707645714]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.89trial/s, best loss: 0.06461751016619098]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.57trial/s, best loss: 0.061133192100795854]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.76trial/s, best loss: 0.07049857966293815]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.98trial/s, best loss: 0.06376620701539804]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.22trial/s, best loss: 0.06345914193789931]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.40trial/s, best loss: 0.06117854513093769]\n",
      "100%|██████████| 500/500 [00:47<00:00, 10.59trial/s, best loss: 0.06995660945844495]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.28trial/s, best loss: 0.0631893417686786] \n",
      "100%|██████████| 500/500 [00:36<00:00, 13.62trial/s, best loss: 0.06412594235881186]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.93trial/s, best loss: 0.061238643541346546]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.08trial/s, best loss: 0.06975118188542931]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.49trial/s, best loss: 0.06354799677081645]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.01trial/s, best loss: 0.06421548314915665]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.62trial/s, best loss: 0.061147162399856875]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.09trial/s, best loss: 0.07109222932184184]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.01trial/s, best loss: 0.06268733498057523]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.61trial/s, best loss: 0.06400310981924913]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.33trial/s, best loss: 0.061317450514544077]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.03trial/s, best loss: 0.07001514998172176]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.60trial/s, best loss: 0.06425369787382573]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.28trial/s, best loss: 0.06450421919781027]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.24trial/s, best loss: 0.06167473194385151]\n",
      "100%|██████████| 500/500 [00:41<00:00, 11.96trial/s, best loss: 0.0699015380042563] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.27trial/s, best loss: 0.06259820854744924]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.77trial/s, best loss: 0.06436151069681896]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.39trial/s, best loss: 0.06134211118332596] \n",
      "100%|██████████| 500/500 [00:41<00:00, 12.07trial/s, best loss: 0.0699599720457495] \n",
      "100%|██████████| 500/500 [00:35<00:00, 14.28trial/s, best loss: 0.06269918802411925]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.45trial/s, best loss: 0.06374716178094185]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.77trial/s, best loss: 0.06220780283167762]\n",
      "100%|██████████| 500/500 [00:35<00:00, 13.91trial/s, best loss: 0.06990422606869015]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.80trial/s, best loss: 0.06334089996611471]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.20trial/s, best loss: 0.06451744470400203]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.44trial/s, best loss: 0.06196845205128135]\n",
      "100%|██████████| 500/500 [00:44<00:00, 11.16trial/s, best loss: 0.0698244231896893] \n",
      "100%|██████████| 500/500 [00:37<00:00, 13.32trial/s, best loss: 0.0629897738149143] \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.81trial/s, best loss: 0.06407408425354685]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.28trial/s, best loss: 0.06093128643789217] \n",
      "100%|██████████| 500/500 [00:40<00:00, 12.30trial/s, best loss: 0.06974932455718348]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.30trial/s, best loss: 0.06291668570542239]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.52trial/s, best loss: 0.06400072086098683]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.00trial/s, best loss: 0.06172341117489391]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.54trial/s, best loss: 0.07007103767416746]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.45trial/s, best loss: 0.06404806791432437]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.78trial/s, best loss: 0.06631628299748708]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.92trial/s, best loss: 0.06110933125267533]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.47trial/s, best loss: 0.06996400555392193]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.50trial/s, best loss: 0.06271396727198979]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.61trial/s, best loss: 0.0638835765227112] \n",
      "100%|██████████| 500/500 [00:30<00:00, 16.22trial/s, best loss: 0.06190233297834545]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.98trial/s, best loss: 0.07024807335526073]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.72trial/s, best loss: 0.06510406307946796]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.61trial/s, best loss: 0.06502364713237506]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.10trial/s, best loss: 0.060924043554086336]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.78trial/s, best loss: 0.06990357175424487]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.57trial/s, best loss: 0.06254272233977642]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.18trial/s, best loss: 0.06420914457181998]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.36trial/s, best loss: 0.06254094209388951]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.33trial/s, best loss: 0.07013362493202]   \n",
      "100%|██████████| 500/500 [00:31<00:00, 15.68trial/s, best loss: 0.06398535444008499]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.59trial/s, best loss: 0.0645020021862912] \n",
      "100%|██████████| 500/500 [00:33<00:00, 15.13trial/s, best loss: 0.06102420707246563]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.62trial/s, best loss: 0.06991560473563024]\n",
      "100%|██████████| 500/500 [00:35<00:00, 14.20trial/s, best loss: 0.06275763769966128]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.22trial/s, best loss: 0.06437067120412691]\n",
      "100%|██████████| 500/500 [00:41<00:00, 11.93trial/s, best loss: 0.06225368594573711]\n",
      "100%|██████████| 500/500 [00:40<00:00, 12.26trial/s, best loss: 0.07005428772738705]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.21trial/s, best loss: 0.06284894279002788]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.83trial/s, best loss: 0.06480819209119526]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.55trial/s, best loss: 0.06213282298665003]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.86trial/s, best loss: 0.07051824830015668]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.50trial/s, best loss: 0.06403287268143629]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.03trial/s, best loss: 0.06396081949661168]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.02trial/s, best loss: 0.06164191640265196]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.48trial/s, best loss: 0.07017980218120057]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.78trial/s, best loss: 0.06271407322131782]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.90trial/s, best loss: 0.06471972651329207]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.66trial/s, best loss: 0.06126702223314947] \n",
      "100%|██████████| 500/500 [00:31<00:00, 15.72trial/s, best loss: 0.07018375860602193]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.66trial/s, best loss: 0.06254328026137453]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.42trial/s, best loss: 0.06453058432473309]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.43trial/s, best loss: 0.06126011194773212]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.49trial/s, best loss: 0.0700546239706924] \n",
      "100%|██████████| 500/500 [00:29<00:00, 16.85trial/s, best loss: 0.06304381657484387]\n",
      "100%|██████████| 500/500 [00:33<00:00, 15.10trial/s, best loss: 0.06362986588400098]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.36trial/s, best loss: 0.06221354603742575]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.34trial/s, best loss: 0.06989673306625367]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.82trial/s, best loss: 0.06252874099515474]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.34trial/s, best loss: 0.06369886116655057]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.80trial/s, best loss: 0.06092349254019469] \n",
      "100%|██████████| 500/500 [00:32<00:00, 15.40trial/s, best loss: 0.06997775400435605]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.53trial/s, best loss: 0.06258232945798868]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.80trial/s, best loss: 0.06368556674727854]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.03trial/s, best loss: 0.06187168079908035] \n",
      "100%|██████████| 500/500 [00:34<00:00, 14.63trial/s, best loss: 0.06975167720761961]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.50trial/s, best loss: 0.06576050549595032]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.66trial/s, best loss: 0.06401293938706021]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.48trial/s, best loss: 0.06371358338352173]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.67trial/s, best loss: 0.07008393758642097]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.72trial/s, best loss: 0.06284006321680964]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.79trial/s, best loss: 0.06364025169928568]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.55trial/s, best loss: 0.06123498740620037]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.55trial/s, best loss: 0.07010194484503761]\n",
      "100%|██████████| 500/500 [00:29<00:00, 16.89trial/s, best loss: 0.06329207770126186]\n",
      "100%|██████████| 500/500 [00:31<00:00, 16.09trial/s, best loss: 0.06463259690685552]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.26trial/s, best loss: 0.06245857009988719]\n",
      "100%|██████████| 500/500 [00:32<00:00, 15.48trial/s, best loss: 0.06991578968043535]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.65trial/s, best loss: 0.0629354741257688] \n",
      "100%|██████████| 500/500 [00:31<00:00, 16.11trial/s, best loss: 0.06439680602160842]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.49trial/s, best loss: 0.0613318889278446]  \n",
      "100%|██████████| 500/500 [00:33<00:00, 14.99trial/s, best loss: 0.07027560375058345]\n",
      "100%|██████████| 500/500 [00:30<00:00, 16.64trial/s, best loss: 0.06339877682986472]\n",
      "100%|██████████| 500/500 [00:31<00:00, 15.78trial/s, best loss: 0.06439194041413149]\n"
     ]
    }
   ],
   "source": [
    "###Run hyperopt 100 times to solve for background\n",
    "max_iters = 500\n",
    "total_background =  pd.DataFrame()\n",
    "perturbs = [1., 1., 1.]\n",
    "for i in range(100):\n",
    "    space = hp.choice('architecture', \n",
    "    [([[0, 0, 1], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "    da_landscape, best = run_hyperopt(max_iters)\n",
    "    \n",
    "    space = hp.choice('architecture', \n",
    "        [([[0, 1, 0], [0, 0, 1]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "    ur_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "    space = hp.choice('architecture', \n",
    "        [([[0, 0, 1], [0, 0, 1]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "    nc_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "    space = hp.choice('architecture', \n",
    "        [([[0, 1, 0], [1, 0, 0]], [[2., hp.uniform('theta1_da', 0.001, 10), hp.uniform('k1_da', 1E-7, 1E-3)], [2., hp.uniform('theta2_da', 0.001, 10), hp.uniform('k2_da', 1E-7, 1E-3)]])])\n",
    "    dc_landscape, best = run_hyperopt(max_iters)\n",
    "\n",
    "    background = pd.concat([dc_landscape, nc_landscape, ur_landscape, da_landscape])\n",
    "    total_background = pd.concat([total_background, background])\n",
    "    #background.to_csv('background.csv', mode='a')\n",
    "    \n",
    "total_background.to_csv('background.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###FIGURE: Landscapes for each architecture\n",
    "landscape = pd.read_csv('single_architecture_run.csv')\n",
    "nc_landscape = landscape.loc[landscape.Circuit == 'No Control']\n",
    "dc_landscape = landscape.loc[landscape.Circuit == 'Dual Control']\n",
    "da_landscape = landscape.loc[landscape.Circuit == 'Downstream Activation']\n",
    "ur_landscape = landscape.loc[landscape.Circuit == 'Upstream Repression']\n",
    "\n",
    "fig, axs = plt.subplots(1,4,figsize=(8, 2))\n",
    "landscapes = [nc_landscape, ur_landscape, da_landscape, dc_landscape]\n",
    "colors = ['Reds','Oranges','Greens', 'Blues']\n",
    "singlecolors = ['red', 'orange', 'green', 'blue']\n",
    "xs = ['k1', 'k1', 'k2', 'k1']\n",
    "ys = ['k2', 'theta1', 'theta2', 'theta1']\n",
    "\n",
    "for i in range(4):\n",
    "    landscape = landscapes[i]\n",
    "    ax = axs[i]\n",
    "    sns.kdeplot(data=landscape, x=xs[i], y =ys[i], cmap=colors[i], shade=True, thresh=0, ax=ax)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(xs[i])\n",
    "    ax.set_ylabel(ys[i])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('landscapes_axes.png', dpi=300)\n",
    "\n",
    "fig, axs = plt.subplots(1,4,figsize=(8, 2))\n",
    "landscapes = [nc_landscape, ur_landscape, da_landscape, dc_landscape]\n",
    "colors = ['Reds','Oranges','Greens', 'Blues']\n",
    "singlecolors = ['red', 'orange', 'green', 'blue']\n",
    "xs = ['k1', 'k1', 'k1', 'k1']\n",
    "ys = ['k2', 'k2', 'k2', 'k2']\n",
    "\n",
    "for i in range(4):\n",
    "    landscape = landscapes[i]\n",
    "    ax = axs[i]\n",
    "    sns.kdeplot(data=landscape, x=xs[i], y =ys[i], cmap=colors[i], shade=True, thresh=0, ax=ax)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(None)\n",
    "    ax.set_ylabel(None)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('landscapes_noaxes.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###FIGURE: Landscapes for each architecture from grid search\n",
    "total_background = pd.read_csv('grid_search_landscape.csv')\n",
    "k1s = total_background.k1.unique()\n",
    "k2s = total_background.k2.unique()\n",
    "theta1s = total_background.theta1.unique()\n",
    "theta2s = total_background.theta2.unique()\n",
    "\n",
    "fig, axs = plt.subplots(1,4,figsize=(8, 2))\n",
    "colors = ['Reds_r','Oranges_r','Greens_r', 'Blues_r']\n",
    "archs = ['No Control', 'Upstream Repression', 'Downstream Activation', 'Dual Control']\n",
    "singlecolors = ['red', 'orange', 'green', 'blue']\n",
    "xs = ['k1', 'k1', 'k2', 'k1']\n",
    "ys = ['k2', 'theta1', 'theta2', 'theta1']\n",
    "xs_actual = [k1s, k1s, k2s, k1s]\n",
    "ys_actual = [k2s, theta1s, theta2s, theta1s]\n",
    "xs_names = ['$k_1$', '$k_1$', '$k_2$', '$k_1$']\n",
    "ys_names = ['$k_2$', r'$\\theta_1$', r'$\\theta_2$', r'$\\theta_1$']\n",
    "for i in range(4):\n",
    "    landscape = total_background.loc[total_background.architecture == archs[i]]\n",
    "    loss_landscape = landscape.pivot_table(index=xs[i], columns=ys[i], values='loss').T.values\n",
    "    ax = axs[i]\n",
    "    contour = ax.contourf(xs_actual[i], ys_actual[i], loss_landscape, cmap=colors[i])\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(xs_names[i])\n",
    "    ax.set_ylabel(ys_names[i])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor(None)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('landscapes_grid_search.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sundials')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07c32881fdddda18fc4efdca8ccb6859d747bae1937efa0776c98adbd36477b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
